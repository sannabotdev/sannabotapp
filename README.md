# ğŸ¤ Sanna â€“ Voice-First AI Assistant for Android

**What OpenClaw does for your desktop, Sanna does for your phone.**

An open-source AI assistant that runs on Android and actually *controls* your phone â€“ not just talks about it. Powered by an LLM agent loop (OpenAI or Claude) that reasons, chains actions, and executes tools until the job is done.

> "Hey Sanna, read my last 3 emails, summarize them, and text the summary to Sarah" â€” just works.

## ğŸ¯ Beta Testing

There are two ways to get a beta release of Sanna:

1. **Build it yourself** â€“ Configure and build the app following the instructions in [DEVELOP.md](DEVELOP.md). Note that this can be time-consuming as you'll need to create many API keys (OpenAI/Claude, Google, Spotify, Picovoice, Slack) for the app to be fully functional.
2. **Request a Test APK** â€“ Write to [sannabot@proton.me](mailto:sannabot@proton.me) and you'll receive a pre-built test APK for beta testing.

## ğŸ“¸ Screenshots

| Driving Mode | Agent Conversation | Scheduler |
|:---:|:---:|:---:|
| <img src="media/driving-mode.png" width="250"> | <img src="media/agent.png" width="250"> | <img src="media/scheduler.png" width="250"> |

| Notification Rules | Skills | Lists |
|:---:|:---:|:---:|
| <img src="media/notifications.png" width="250"> | <img src="media/skills.png" width="250"> | <img src="media/lists.png" width="250"> |

## âœ¨ Highlights

- **ğŸ—£ï¸ Voice-first** â€“ Wake word ("Hey Sanna") â†’ Speech-to-Text â†’ LLM agent â†’ Text-to-Speech, fully hands-free
- **ğŸª„ Persona (SOUL)** â€“ Give Sanna a personality. Define her tone, style, and character in plain text. Editable in Settings, supports voice dictation (STT), persists across reinstalls.
- **ğŸ§  Personal Memory** â€“ Sanna remembers facts about you: name, family, job, home, hobbies, anniversaries, birthdays, important events. When you mention a personal detail, she writes it into a structured personal memory that is injected into every prompt. Curated automatically by the LLM â€“ deduplicated and condensed on every update.
- **ğŸ“ Skills are Markdown** â€“ Drop a `SKILL.md` in a folder, the agent learns a new capability. No code changes.
- **ğŸ”„ Agentic tool loop** â€“ LLM â†’ tool call â†’ result â†’ back to LLM, until final answer. Multi-step reasoning out of the box.
- **ğŸ“‹ Local list management** â€“ Shopping lists, to-dos, packing lists â€“ stored on-device, fully offline, no cloud.
- **â° Sub-agent scheduler** â€“ Schedule natural-language tasks ("Every Monday at 9am, brief me on today's calendar via SMS"). A real LLM executes them â€“ not a dumb cron job.
- **ğŸ”” Notification rules** â€“ Define what happens when a notification arrives: read it aloud, auto-reply, play an alarm â€“ each rule spawns its own LLM sub-agent with full tool access.
- **ğŸ¤– UI Automation** â€“ Controls other apps via Android Accessibility Services. An LLM sub-agent reads the UI tree, clicks buttons, types text â€“ e.g. sends WhatsApp messages without any API.
- **ğŸ§© Learning Accessibility** â€“ The system learns from every UI interaction: After each run, successful and failed flows are condensed into natural language and stored per app. On the next task for the same app, these hints are automatically injected into the system prompt, enabling the agent to learn from past experiences and improve over time.
- **ğŸš— Driving mode** â€“ Short spoken responses, auto-reads incoming notifications, optimized for hands-free use.
- **ğŸ”’ No backend needed** â€“ OAuth flows use PKCE. All data stays on your device.

## ğŸ“¦ 16 Built-in Skills

| Skill | What it does |
|-------|-------------|
| ğŸ“§ Gmail | Read, search, send, and reply to emails |
| ğŸ“… Calendar | Query and create Google Calendar events |
| âœ… Google Tasks | Manage task lists and items |
| ğŸ’¬ Slack | Read/send messages, manage DMs, set status |
| ğŸµ Spotify | Playback control via Web API |
| ğŸ“± WhatsApp | Send messages via Android Intents |
| ğŸ’¬ SMS | Send SMS directly in the background |
| ğŸ“ Phone | Make calls |
| ğŸ‘¤ Contacts | Search and query contacts |
| ğŸ—ºï¸ Google Maps | Start navigation |
| ğŸ”” Notifications | Rule-based notification handling with LLM sub-agents |
| â° Scheduler | Autonomous scheduled tasks with sub-agents |
| â±ï¸ Timer | Countdown timers and stopwatches with acoustic alarms |
| ğŸ“ Journal | Create and manage journal entries to track activities, events, and notes |
| ğŸ“‹ Lists | Manage local lists (shopping, to-do, packing) â€“ fully offline |
| ğŸŒ¤ï¸ Weather | Current weather & forecasts via wttr.in / Open-Meteo â€“ no API key |

## ğŸ’¡ Usage Examples

Sanna is conversational â€“ just speak or type naturally. Here are some things you can do:

### ğŸ“§ Email

| You say | What happens |
|---------|-------------|
| "Read my last 3 emails" | Fetches Gmail inbox, summarises sender + subject |
| "Search for emails from the bank" | Searches Gmail by sender |
| "Reply to the last email: sounds good, thanks" | Sends a reply to the most recent message |
| "Send an email to sarah@example.com about the meeting tomorrow" | Composes and sends a new email |

### ğŸ“… Calendar & Tasks

| You say | What happens |
|---------|-------------|
| "What's on my calendar today?" | Lists today's Google Calendar events |
| "Am I free at 3 PM?" | Checks for conflicts |
| "Create an appointment: dentist Friday at 10 AM" | Creates a calendar event |
| "What are my open tasks?" | Lists Google Tasks |
| "Add 'buy flowers' to my tasks" | Creates a new task item |

### â° Scheduler (Background Sub-Agents)

The scheduler creates autonomous tasks that run in the background at the specified time. Each task is executed by an independent LLM sub-agent with full tool access.

| You say | What happens |
|---------|-------------|
| "Remind me in 10 minutes about the pizza" | Plays an alarm + TTS at the scheduled time |
| "Every morning at 8, read me today's calendar" | Recurring: fetches calendar and speaks via TTS daily |
| "Send an SMS to +43660123456 at 2 PM: on my way" | One-time: sends the SMS at 14:00 |
| "Every Monday at 9, brief me on my emails" | Recurring: fetches Gmail and speaks a summary weekly |
| "What do I have scheduled?" | Lists all active schedules |
| "Delete the pizza reminder" | Removes a schedule by description |

> The sub-agent is a full LLM with all tools â€“ it can read emails, send messages, play sounds, and speak via TTS, all without you touching the phone.

### ğŸ”” Notifications (Rule-Based Sub-Agents)

Notification rules tell Sanna what to do when a notification arrives from a specific app. Each rule has an **instruction** (what the sub-agent should do) and an optional **condition** (when it should trigger, evaluated by the LLM).

| You say | What happens |
|---------|-------------|
| "Read WhatsApp messages to me" | Creates a catch-all rule: every WhatsApp notification is read aloud via TTS |
| "When my boss writes an email, read the full content" | Creates a conditional rule: only triggers when the LLM determines the sender matches |
| "When someone from my team writes on Slack and it's urgent, play an alarm" | Conditional: LLM evaluates sender + content semantically |
| "Auto-reply to WhatsApp messages from my partner with 'I'm driving'" | Sub-agent uses the WhatsApp tool to send a reply |
| "Stop WhatsApp notifications" | Removes all rules for WhatsApp |
| "What notification rules do I have?" | Lists all active rules with conditions and instructions |

**How it works:**
1. You tell Sanna to subscribe to an app (e.g. "read me WhatsApp messages")
2. Sanna creates a notification rule with an instruction and optional condition
3. When a notification arrives, the LLM evaluates the condition against the notification content
4. If it matches, an independent sub-agent executes the instruction with full tool access
5. If no condition matches, nothing happens â€“ the notification is silently skipped
6. The main conversation pipeline stays free the entire time

> Conditions are evaluated **semantically** by the LLM â€“ "The sender is my boss" works even without specifying an exact name. Multiple rules can exist for the same app, with the most specific (conditional) rule taking priority.

### â±ï¸ Timer

Simple countdown timers and stopwatches with acoustic alarms. Examples: "Egg timer 20 seconds", "Set a timer for 3 minutes", "Start stopwatch for running", "What timers are running?"

### ğŸ“ Journal

Create and manage journal entries to track activities, events, and notes. Stored locally on-device. Examples: "Make an entry in the journal that I went jogging today", "Show me my journal entries", "What journal entries are in the Work category?"

### ğŸ“‹ Lists

Lists are stored locally on-device â€“ no internet, no cloud, no account needed.

| You say | What happens |
|---------|-------------|
| "Add milk and bread to my shopping list" | Creates the list if it doesn't exist, adds items |
| "What's on my shopping list?" | Reads the list aloud |
| "Remove bread from the shopping list" | Deletes the item |
| "Create a packing list for vacation" | Creates a new named list |
| "Check off milk" | Marks an item as done |

### ğŸ’¬ Messaging

| You say | What happens |
|---------|-------------|
| "WhatsApp John: I'll be there in 10 minutes" | Looks up contact, sends via WhatsApp |
| "Text Mom: running late" | Sends an SMS (background, no UI) |
| "Call the dentist" | Looks up contact, initiates phone call |

### ğŸµ Music & Media

| You say | What happens |
|---------|-------------|
| "Play some jazz on Spotify" | Searches and plays via Spotify Web API |
| "Next song" / "Pause" / "Resume" | Playback control |
| "Set volume to 60 percent" | Adjusts media volume |

### ğŸ—ºï¸ Navigation & Weather

| You say | What happens |
|---------|-------------|
| "Navigate to the airport" | Opens Google Maps turn-by-turn |
| "Will it rain tomorrow?" | Weather forecast for current GPS location |
| "What's the weather in Vienna?" | Weather for a specific city |

### ğŸ¤– UI Automation (Learning Accessibility)

Sanna can control other Android apps via Accessibility Services â€“ no API access needed. An LLM sub-agent reads the UI tree, clicks buttons, types text, and navigates through apps.

**What makes it special: The system learns from every interaction.**

| You say | What happens |
|---------|-------------|
| "Send a WhatsApp message to John: I'll be there in 10 minutes" | Opens WhatsApp, finds contact, types message, sends |
| "Post 'Hello world' on Twitter" | Opens Twitter/X, navigates to post editor, types text, sends |
| "Open Instagram and like the first post" | Opens Instagram, finds first post, clicks like button |

**How the learning works:**

1. **After each run** (successful or failed), an LLM analyzes the complete interaction history
2. **Condensation**: The full history (accessibility trees + actions) is compressed into 3â€“4 concise paragraphs of natural language
3. **Storage**: These hints are stored per app package (e.g., `com.whatsapp`, `com.twitter.android`)
4. **Reuse**: On the next task for the same app, the stored hints are automatically injected into the system prompt
5. **Continuous improvement**: The agent learns from past successes and failures â€“ "âœ… To achieve X: navigate to home, then click 'Y'..." or "âŒ Attempting X via Y did NOT work because..."

**Example:**
- **First WhatsApp message**: The agent must explore WhatsApp's UI structure
- **Second WhatsApp message**: The agent has already learned how to navigate WhatsApp and send messages â†’ faster and more reliable
- **After multiple runs**: The agent knows successful flows and avoids known pitfalls

> The hints contain **no node IDs** (those are ephemeral), but describe flows in natural language using button labels and UI text. This makes them robust against UI changes.

### ğŸ”— Multi-Step Chains

The agent loop means you can chain actions naturally:

| You say | What happens |
|---------|-------------|
| "Read my last email and reply with 'sounds good'" | Fetches email â†’ composes reply â†’ sends |
| "What's on my calendar tomorrow? Text the summary to Mom" | Calendar â†’ format â†’ SMS |
| "Add everything from my shopping list to a new Google Task list" | Reads file â†’ creates tasks |

## ğŸš— Driving Mode

Toggle driving mode with one tap. Sanna becomes a fully hands-free co-pilot:

| Feature | How it works |
|---------|-------------|
| **Voice-only interaction** | Wake word ("Hey Sanna") â†’ speak your request â†’ hear the answer. No screen touch needed. |
| **Ultra-short answers** | The LLM is instructed to reply in 1â€“2 sentences max â€“ every word is read aloud, so brevity saves attention. |
| **Auto-read notifications** | Subscribe to WhatsApp, Telegram, SMS, email, etc. â€“ incoming messages are summarised and spoken automatically. |
| **Navigation** | "Navigate to the airport" â†’ opens Google Maps turn-by-turn navigation instantly. |
| **Calls & messages** | "Call Mom" / "Text Sarah: running 10 minutes late" / "WhatsApp John: on my way" â€“ contact lookup, confirmation, and send, all by voice. |
| **Music control** | "Play Rammstein on Spotify" / "Next song" / "Pause" â€“ full Spotify playback control via voice. |
| **Volume control** | "Set volume to 80 percent" / "Turn it down" â€“ adjusts media volume directly. |
| **Calendar & schedule** | "What's my next appointment?" / "Am I free at 3 PM?" â€“ reads your Google Calendar aloud. |
| **Weather** | "Will it rain tomorrow?" / "What's the weather?" â€“ fetches weather for your GPS location or any city. |
| **Screen stays on** | Display never sleeps while driving mode is active â€“ no unlock needed. |
| **Email triage** | "Read my latest emails" / "Reply to the last email: sounds good" â€“ hands-free Gmail access. |

> Driving mode optimises every part of the pipeline: the system prompt enforces brevity,
> all responses are auto-spoken via TTS, and notifications from subscribed apps are read aloud in real time.

## ğŸ—ï¸ Architecture

```
Wake Word (Picovoice) â†’ STT â†’ LLM Agent Loop â†’ Tool Execution â†’ TTS
                                    â†•
                           SKILL.md files (auto-discovered)
                                    â†•
                         Tools: intent, http, tts, device, file_storage,
                         sms, query, scheduler, notifications, accessibility,
                         timer, journal, app_search, beep, personal_memory
```

**Sub-agent architecture:** Both the scheduler and notification system spawn independent LLM sub-agents. These run in their own tool loop with full tool access, completely separate from the main conversation pipeline. The main pipeline stays free for user interaction at all times.

```
Main Pipeline (user conversation)
    â”œâ”€â”€ Scheduler Sub-Agent (time-triggered, background)
    â”œâ”€â”€ Notification Sub-Agent (event-triggered, per notification)
    â””â”€â”€ Accessibility Sub-Agent (UI automation, per task)
        â””â”€â”€ AccessibilityHintStore (learned hints per app, persisted in AsyncStorage)
```

**Persona & Memory injection:** Soul (persona) and Personal Memory are loaded from on-device storage and injected into the system prompt of every agent â€“ main pipeline, notification sub-agent, and scheduler sub-agent. The accessibility sub-agent receives them read-only (no memory writes during UI automation). Memory writes (`memory_personal_upsert`) are restricted to the main loop only.

**Learning mechanism:** After each accessibility automation run, the full interaction history (accessibility trees + actions) is condensed by an LLM into natural-language hints. These hints are stored per app package and automatically injected into future runs for the same app, enabling the agent to learn from past experiences and improve over time.

- **React Native** + native **Kotlin** modules for Android-specific features
- **LLM providers**: OpenAI (`gpt-4o`) or Anthropic Claude â€“ swap with one config line
- **Skill auto-discovery**: Metro's `require.context()` scans `assets/skills/*/SKILL.md` at build time
- **OAuth2 PKCE**: Google, Spotify, Slack â€“ no server, no client secret

## ğŸš€ Quick Start

1. Clone the repo
2. `cp local.config.example.ts local.config.ts` and add your API keys
3. `npm install && npm run android`

See [DEVELOP.md](DEVELOP.md) for detailed credential setup (API keys, OAuth, etc.) and building instructions.

## ğŸ¤ Adding a Skill

There are two ways to add a skill â€“ **no rebuild required** for the upload path:

### Option 1: Upload at runtime (no rebuild)

Open **Settings â†’ Skills â†’ Upload Skill**, pick any `.md` file from your device, and the skill is live immediately. Uploaded skills are persisted in on-device storage and survive app restarts. You can also delete them from the same screen.

### Option 2: Bundle at build time

Create `assets/skills/your-skill/SKILL.md` â€“ Metro auto-discovers it on the next build:

```markdown
---
name: your-skill
description: What this skill does
---
# Your Skill

## Tool: http

### Do something

```json
{
  "method": "GET",
  "url": "https://api.example.com/endpoint",
  "auth_provider": "google"
}
```

Both paths use the same `SKILL.md` format. The agent picks up the skill automatically â€“ no code changes needed.

## ğŸ“„ License

MIT
